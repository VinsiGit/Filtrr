{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:28:40.994018200Z",
     "start_time": "2024-03-05T16:28:40.985774Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from googletrans import Translator\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    This class contains the basic preprocessor pipeline that's used to extract keywords themed around skills.\n",
    "\n",
    "    Attributes:\n",
    "        _dir_skill (str): The directory path where the skill preprocessor model is stored.\n",
    "        _dir_knowledge (str): The directory path where the knowledge preprocessor model is stored.\n",
    "        _window_size (int): The window size used in the preprocessor pipeline.\n",
    "        __words (set): Set of English words.\n",
    "        __lemmatizer (WordNetLemmatizer): Instance of WordNetLemmatizer for lemmatization.\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initializes the Preprocessor class with default or user-specified parameters.\n",
    "        preprocess_input: Preprocesses input and gives skills & knowledge back.\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size: int = 8, preprocessor_dir_skill: str = \"./preprocessor/skill/\", preprocessor_dir_knowledge: str = \"./preprocessor/knowledge/\"):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessor class with default or user-specified parameters.\n",
    "\n",
    "        Args:\n",
    "            window_size (int): The window size used in the preprocessor pipeline. Default is 8.\n",
    "            preprocessor_dir_skill (str): The directory path where the skill preprocessor model is stored.\n",
    "                Default is \"./preprocessor/skill/\".\n",
    "            preprocessor_dir_knowledge (str): The directory path where the knowledge preprocessor model is stored.\n",
    "                Default is \"./preprocessor/knowledge/\".\n",
    "        \"\"\"\n",
    "        self._window_size = window_size\n",
    "        \n",
    "        self._dir_skill = preprocessor_dir_skill\n",
    "        self._dir_knowledge = preprocessor_dir_knowledge\n",
    "        self._skill_preprocessor = self.__load_model(dir=self._dir_skill, model_name=\"jjzha/jobbert_skill_extraction\")\n",
    "        self._knowledge_preprocessor = self.__load_model(dir=self._dir_knowledge, model_name=\"jjzha/jobbert_knowledge_extraction\")\n",
    "\n",
    "        # region Initialize NLTK resources\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('words', quiet=True)\n",
    "        # endregion\n",
    "        \n",
    "        self.__words = set(words.words('en') + stopwords.words('english'))\n",
    "        self.__lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __load_model(self, dir: str, model_name: str):\n",
    "        \"\"\"\n",
    "        Loads a preprocessor model from the specified directory.\n",
    "\n",
    "        Args:\n",
    "            dir (str): The directory path where the preprocessor model is stored.\n",
    "            model_name (str): The name of the preprocessor model to load.\n",
    "\n",
    "        Returns:\n",
    "            object: The preprocessor model loaded from the specified directory.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dir) or not os.listdir(dir):\n",
    "            token_classifier = pipeline(model=model_name, aggregation_strategy=\"first\")\n",
    "            token_classifier.save_pretrained(dir)\n",
    "        return pipeline(model=dir, task=\"ner\")\n",
    "\n",
    "    def __regex_privacy(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Applies regex patterns to remove privacy-sensitive information from text.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text.\n",
    "    \n",
    "        Returns:\n",
    "            str: The text with privacy-sensitive information removed.\n",
    "        \"\"\"\n",
    "        # Define regex patterns to remove privacy-sensitive information\n",
    "        patterns = [\n",
    "            r'\\r',  # Remove carriage return\n",
    "            r'\\n',  # Remove newline\n",
    "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)',  # Remove URLs\n",
    "            r'^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$',  # Remove phone numbers\n",
    "            r'\\d',  # Remove digits\n",
    "            r'-',  # Remove hyphens\n",
    "            r'[\\[\\](){}<>]',  # Remove brackets and parentheses\n",
    "            r'[,.;:!?&+_\\/]'  # Remove common punctuation\n",
    "        ]\n",
    "    \n",
    "        # Apply each pattern to the text\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def __clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans the text by removing privacy-sensitive information and multiple spaces.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "        \"\"\"\n",
    "        text_multi_space = text.replace(' ', '_')\n",
    "        text_cleaned = self.__regex_privacy(text_multi_space)\n",
    "        return text_cleaned\n",
    "\n",
    "    def __translate_to_english(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Translates text to English using Google Translate API.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text.\n",
    "\n",
    "        Returns:\n",
    "            str: The translated text in English.\n",
    "        \"\"\"\n",
    "        translator = Translator()\n",
    "        translated_text = translator.translate(text, src='auto', dest='en').text\n",
    "        return translated_text\n",
    "\n",
    "    def __tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes, lemmatizes, and stems the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: The list of tokens after tokenization & lemmatization.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        lemmatized_tokens = [self.__lemmatizer.lemmatize(token) for token in tokens]\n",
    "        filtered_tokens = [token for token in lemmatized_tokens if token not in self.__words]\n",
    "        return filtered_tokens\n",
    "\n",
    "    def preprocess(self, email: any) -> dict:\n",
    "        \"\"\"\n",
    "        Preprocesses the input email to extract skills and knowledge.\n",
    "        Args:\n",
    "            email (dict): The input email to be preprocessed.\n",
    "        Returns:\n",
    "            dict: A dictionary containing lists of dictionaries,\n",
    "                where each dictionary represents a skill or knowledge term with its corresponding score and type.\n",
    "                The dictionary has three keys: 'skill', 'knowledge', and 'keywords', each containing a list of dictionaries.\n",
    "        \"\"\"\n",
    "        text = email.get('text_body', '')\n",
    "        clean_text = self.__clean_text(text)\n",
    "        text_en = self.__translate_to_english(clean_text)\n",
    "        text_en_clean = self.__clean_text(text_en)\n",
    "        tokens = self.__tokenize(text_en_clean)\n",
    "        \n",
    "        unique_tokens = list(set(tokens))\n",
    "\n",
    "        # Process tokens using skill_preprocessor\n",
    "        skill_output = [self._skill_preprocessor(' '.join(tokens[i - self._window_size:i + self._window_size])) for i in range(self._window_size, len(tokens), self._window_size)]\n",
    "        transformed_skill_output = [{'word': item['word'], 'type': 'skill'} for sublist in skill_output for item in sublist if sublist]\n",
    "\n",
    "        # Process tokens using knowledge_preprocessor\n",
    "        knowledge_output = [self._knowledge_preprocessor(' '.join(tokens[i - self._window_size:i + self._window_size])) for i in range(self._window_size, len(tokens), self._window_size)]\n",
    "        transformed_knowledge_output = [{'word': item['word'], 'type': 'knowledge'} for sublist in knowledge_output for item in sublist if sublist]\n",
    "\n",
    "        email['skill'] = transformed_skill_output\n",
    "        email['knowledge'] = transformed_knowledge_output\n",
    "        email['keywords'] = unique_tokens\n",
    "        email['rating'] = None\n",
    "\n",
    "        return email"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:28:41.017578300Z",
     "start_time": "2024-03-05T16:28:40.994018200Z"
    }
   },
   "id": "8da47ed5078be26b"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "email =  {\n",
    "    \"item_id\": 0,\n",
    "    \"sender\": \"a1d400258b5c6e3d97307b2c949ffe01fe0aa27ab02ef1c351a7bfa6e0f300a3\",\n",
    "    \"sender_email\": \"6fedc8e86e6e05504fefcdce51f8f73b69f5fd104c23dc5e9dba6c64e5536ffd\",\n",
    "    \"datetime_received\": 1707207587000,\n",
    "    \"sensitivity\": \"Normal\",\n",
    "    \"subject\": \"Datawarehousing Specialist (4. Expert (10+)) SWI000876 - For Swift\",\n",
    "    \"text_body\": \"ENKEL RECHTSTREEKS, GEEN TUSSENPARTIJEN AUB\\r\\n\\r\\nVOORRANG VASTE MEDEWERKERS\\r\\n\\r\\n\\r\\n\\r\\nHallo collega\\u2019s,\\r\\n\\r\\n\\r\\n\\r\\nVoor Swift zoeken we een Datawarehousing Specialist (4. Expert (10+)) SWI000876 die voldoet aan volgende beschrijving:\\r\\n\\r\\n\\r\\n\\r\\nUiterste reactiedatum: 16/02/2024\\r\\n\\r\\nGewenste startdatum: 01/03/2024\\r\\n\\r\\nEinddatum: 31/08/2024\\r\\n\\r\\n\\r\\n\\r\\nReferentie: SWI000876\\r\\n\\r\\nTitel: Datawarehousing Specialist (4. Expert (10+)) SWI000876\\r\\n\\r\\nLocatie: THE NETHERLANDS - ZOETERWOUDE (ENERGIEWEG 33, 2382 NC ZOETERWOUDE, NEDERLAND)\\r\\n\\r\\nStatus: Gepubliceerd\\r\\n\\r\\nType contract: Time & material\\r\\n\\r\\nCategorie: Niet van toepassing\\r\\n\\r\\nAantal personen: 1\\r\\n\\r\\nAfdeling: Human Resource (HR)\\r\\n\\r\\n\\r\\n\\r\\nOmschrijving\\r\\n\\r\\n\\r\\n\\r\\nThe project for which the candidate will be assigned is called Digital Dashboards, having the goal of building executive dashboarding for Swift. The ideal candidate will have an extensive background and expertise in MS Power BI, with both the ability to design the data model, as well as the reports and dashboards.\\r\\n\\r\\nThe candidate will join the project team and will have a key role to play, not just delivering on the scope of the project, but also training the team on that technology since it is still quite new at Swift.\\r\\n\\r\\n\\r\\nOpdracht informatie\\r\\n\\r\\n\\r\\n\\r\\nProjectnaam: Digital Dashboards\\r\\n\\r\\nWerkregime: Voltijds\\r\\n\\r\\n\\r\\n\\r\\nVaardigheden\\r\\n\\r\\n\\r\\n\\r\\nSPECIFIEKE VAARDIGHEDEN\\r\\n\\r\\n\\r\\n\\r\\nData Modeling: Expert (10+)\\r\\n\\r\\nETL Development: Expert (10+)\\r\\n\\r\\nMS Power Bi: Expert (10+)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nIndien jullie geschikte kandidaten hebben ontvang ik graag hun beschikbaarheid, CV en kostprijs.\\r\\n\\r\\n\\r\\n\\r\\nAlvast hartelijk bedankt.\\r\\n\\r\\n\\r\\n[signature_1929168496]\\r\\n\\r\\nChannice \\r\\n\\r\\nExecutive Assistant - Business and sales support\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nDe Cronos Groep nv\\r\\n\\r\\nVeldkant 33A, 2550 Kontich\\r\\n\\r\\n\\r\\n\",\n",
    "    \"label\": \"BI_ENGINEER\",\n",
    "    \"keywords\": [\n",
    "      \"Datawarehousing Specialist\",\n",
    "      \"MS Power BI\",\n",
    "      \"Data Modeling\",\n",
    "      \"ETL Development\"\n",
    "    ]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:28:41.020584100Z",
     "start_time": "2024-03-05T16:28:41.017578300Z"
    }
   },
   "id": "c982bb06c5c6eb44"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(3)\n",
    "# loads in 8s and 788ms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:28:49.808920600Z",
     "start_time": "2024-03-05T16:28:41.020584100Z"
    }
   },
   "id": "c25a41f523be4d3a"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'item_id': 0,\n 'sender': 'a1d400258b5c6e3d97307b2c949ffe01fe0aa27ab02ef1c351a7bfa6e0f300a3',\n 'sender_email': '6fedc8e86e6e05504fefcdce51f8f73b69f5fd104c23dc5e9dba6c64e5536ffd',\n 'datetime_received': 1707207587000,\n 'sensitivity': 'Normal',\n 'subject': 'Datawarehousing Specialist (4. Expert (10+)) SWI000876 - For Swift',\n 'text_body': 'ENKEL RECHTSTREEKS, GEEN TUSSENPARTIJEN AUB\\r\\n\\r\\nVOORRANG VASTE MEDEWERKERS\\r\\n\\r\\n\\r\\n\\r\\nHallo collegaâ€™s,\\r\\n\\r\\n\\r\\n\\r\\nVoor Swift zoeken we een Datawarehousing Specialist (4. Expert (10+)) SWI000876 die voldoet aan volgende beschrijving:\\r\\n\\r\\n\\r\\n\\r\\nUiterste reactiedatum: 16/02/2024\\r\\n\\r\\nGewenste startdatum: 01/03/2024\\r\\n\\r\\nEinddatum: 31/08/2024\\r\\n\\r\\n\\r\\n\\r\\nReferentie: SWI000876\\r\\n\\r\\nTitel: Datawarehousing Specialist (4. Expert (10+)) SWI000876\\r\\n\\r\\nLocatie: THE NETHERLANDS - ZOETERWOUDE (ENERGIEWEG 33, 2382 NC ZOETERWOUDE, NEDERLAND)\\r\\n\\r\\nStatus: Gepubliceerd\\r\\n\\r\\nType contract: Time & material\\r\\n\\r\\nCategorie: Niet van toepassing\\r\\n\\r\\nAantal personen: 1\\r\\n\\r\\nAfdeling: Human Resource (HR)\\r\\n\\r\\n\\r\\n\\r\\nOmschrijving\\r\\n\\r\\n\\r\\n\\r\\nThe project for which the candidate will be assigned is called Digital Dashboards, having the goal of building executive dashboarding for Swift. The ideal candidate will have an extensive background and expertise in MS Power BI, with both the ability to design the data model, as well as the reports and dashboards.\\r\\n\\r\\nThe candidate will join the project team and will have a key role to play, not just delivering on the scope of the project, but also training the team on that technology since it is still quite new at Swift.\\r\\n\\r\\n\\r\\nOpdracht informatie\\r\\n\\r\\n\\r\\n\\r\\nProjectnaam: Digital Dashboards\\r\\n\\r\\nWerkregime: Voltijds\\r\\n\\r\\n\\r\\n\\r\\nVaardigheden\\r\\n\\r\\n\\r\\n\\r\\nSPECIFIEKE VAARDIGHEDEN\\r\\n\\r\\n\\r\\n\\r\\nData Modeling: Expert (10+)\\r\\n\\r\\nETL Development: Expert (10+)\\r\\n\\r\\nMS Power Bi: Expert (10+)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nIndien jullie geschikte kandidaten hebben ontvang ik graag hun beschikbaarheid, CV en kostprijs.\\r\\n\\r\\n\\r\\n\\r\\nAlvast hartelijk bedankt.\\r\\n\\r\\n\\r\\n[signature_1929168496]\\r\\n\\r\\nChannice \\r\\n\\r\\nExecutive Assistant - Business and sales support\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nDe Cronos Groep nv\\r\\n\\r\\nVeldkant 33A, 2550 Kontich\\r\\n\\r\\n\\r\\n',\n 'label': 'BI_ENGINEER',\n 'keywords': ['bi',\n  'hr',\n  'cv',\n  'netherlands',\n  'theability',\n  'zoeterwoude',\n  'energewoude',\n  'exualive',\n  'quitenew',\n  'kontich',\n  'called',\n  'cronos',\n  'expertise',\n  'swi',\n  'nederland',\n  'publishedcategory',\n  'etl',\n  'delivering',\n  'datawarehousing',\n  'dashboarding',\n  'groupnv'],\n 'skill': [{'word': 'delivering', 'type': 'skill'},\n  {'word': 'quite', 'type': 'skill'},\n  {'word': '##ne', 'type': 'skill'},\n  {'word': '##w', 'type': 'skill'},\n  {'word': 'et', 'type': 'skill'},\n  {'word': '##l', 'type': 'skill'}],\n 'knowledge': [{'word': 'data', 'type': 'knowledge'},\n  {'word': '##ware', 'type': 'knowledge'},\n  {'word': '##hou', 'type': 'knowledge'},\n  {'word': '##sing', 'type': 'knowledge'},\n  {'word': 's', 'type': 'knowledge'},\n  {'word': '##wi', 'type': 'knowledge'},\n  {'word': 's', 'type': 'knowledge'},\n  {'word': '##wi', 'type': 'knowledge'},\n  {'word': 'data', 'type': 'knowledge'},\n  {'word': '##ware', 'type': 'knowledge'},\n  {'word': '##hou', 'type': 'knowledge'},\n  {'word': '##sing', 'type': 'knowledge'},\n  {'word': 's', 'type': 'knowledge'},\n  {'word': '##wi', 'type': 'knowledge'},\n  {'word': 'data', 'type': 'knowledge'},\n  {'word': '##ware', 'type': 'knowledge'},\n  {'word': '##hou', 'type': 'knowledge'},\n  {'word': '##sing', 'type': 'knowledge'},\n  {'word': 's', 'type': 'knowledge'},\n  {'word': '##wi', 'type': 'knowledge'},\n  {'word': 'net', 'type': 'knowledge'},\n  {'word': '##her', 'type': 'knowledge'},\n  {'word': '##lands', 'type': 'knowledge'},\n  {'word': '##l', 'type': 'knowledge'},\n  {'word': 'bi', 'type': 'knowledge'},\n  {'word': 'c', 'type': 'knowledge'},\n  {'word': '##v', 'type': 'knowledge'},\n  {'word': 'c', 'type': 'knowledge'}],\n 'rating': None}"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.preprocess(email)\n",
    "# classification takes 869 ms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:28:50.678227100Z",
     "start_time": "2024-03-05T16:28:49.808920600Z"
    }
   },
   "id": "7faebae92cfc541c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
