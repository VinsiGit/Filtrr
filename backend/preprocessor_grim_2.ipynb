{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T08:28:10.809491800Z",
     "start_time": "2024-03-04T08:28:10.793887Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grimm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Grimm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from joblib import dump, load\n",
    "from transformers import pipeline, Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import nltk\n",
    "#TODO: zorgen dat deze niet elke keer opnieuw downloaden\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8da47ed5078be26b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T08:28:11.310837600Z",
     "start_time": "2024-03-04T08:28:11.295166700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    This class contains the basic preprocessor pipeline that's used to extract keywords themed around skills.\n",
    "\n",
    "    Attributes:\n",
    "        __dir_skill (str): The directory path where the skill preprocessor model is stored.\n",
    "        __dir_knowledge (str): The directory path where the knowledge preprocessor model is stored.\n",
    "        _window_size (int): The window size used in the preprocessor pipeline.\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initializes the Preprocessor class with default or user-specified parameters.\n",
    "        preprocess_input: Preprocesses input and gives skills & knowledge back\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size: int = 8, preprocessor_dir_skill: str = \"./preprocessor/skill/\", preprocessor_dir_knowledge: str = \"./preprocessor/knowledge/\"):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessor class with default or user-specified parameters.\n",
    "\n",
    "        Args:\n",
    "            window_size (int): The window size used in the preprocessor pipeline. Default is 8.\n",
    "            preprocessor_dir_skill (str): The directory path where the skill preprocessor model is stored.\n",
    "                Default is \"./preprocessor/skill/\".\n",
    "            preprocessor_dir_knowledge (str): The directory path where the knowledge preprocessor model is stored.\n",
    "                Default is \"./preprocessor/knowledge/\".\n",
    "        \"\"\"\n",
    "        self._dir_skill = preprocessor_dir_skill\n",
    "        self._dir_knowledge = preprocessor_dir_knowledge\n",
    "        self._window_size = window_size\n",
    "        self._skill_preprocessor = self.__load_model(dir=self._dir_skill, model_name=\"jjzha/jobbert_skill_extraction\")\n",
    "        self._knowledge_preprocessor = self.__load_model(dir=self._dir_knowledge, model_name=\"jjzha/jobbert_knowledge_extraction\")\n",
    "        \n",
    "    def __load_model(self, dir:str, model_name:str):\n",
    "        \"\"\"\n",
    "        Loads a preprocessor model from the specified directory.\n",
    "\n",
    "        Args:\n",
    "            dir (str): The directory path where the preprocessor model is stored.\n",
    "            model_name (str): The name of the preprocessor model to load.\n",
    "\n",
    "        Returns:\n",
    "            object: The preprocessor model loaded from the specified directory.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dir) or not os.listdir(dir):\n",
    "            token_classifier = pipeline(model=model_name, aggregation_strategy=\"first\")\n",
    "            token_classifier.save_pretrained(dir)\n",
    "        return pipeline(model=dir, task=\"ner\")   \n",
    "    \n",
    "    def preprocess_parallel(self, emails):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(self.process_text, emails))\n",
    "\n",
    "        json_data = {}\n",
    "\n",
    "        for i, result in enumerate(results, 1):\n",
    "            keywords = []\n",
    "            for entity in result['entities']:\n",
    "                if entity['score'] > 0.85:\n",
    "                    keywords.append({'word': self.postprocess_text(entity['word']), 'score': entity['score']})\n",
    "            json_data[f'mail_{i}'] = {'title': result['title'], 'keywords': keywords}\n",
    "\n",
    "        return json_data\n",
    "    \n",
    "    def postprocess_text(self, text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words_english = set(stopwords.words(\"english\"))\n",
    "        stop_words_dutch = set(stopwords.words(\"dutch\"))\n",
    "        words = re.findall(r'\\b\\w+\\b|\\s+', text)\n",
    "        cleaned_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.strip().isalpha() and word.lower().strip() not in stop_words_english.union(stop_words_dutch)]\n",
    "        cleaned_text = ' '.join(cleaned_words)\n",
    "        return cleaned_text\n",
    "    \n",
    "    def process_text(self, email):\n",
    "        sentences = email['text_body'].split('\\n')\n",
    "        entities = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            output_skills = self._skill_preprocessor(sentence)\n",
    "            print(output_skills)\n",
    "            for result in output_skills:\n",
    "                if result.get(\"entity_group\"):\n",
    "                    result[\"entity_type\"] = \"Skill\"\n",
    "                    del result[\"entity_group\"]\n",
    "                    entities.append(result)\n",
    "\n",
    "            output_knowledge = self._knowledge_preprocessor(sentence)\n",
    "            for result in output_knowledge:\n",
    "                if result.get(\"entity_group\"):\n",
    "                    result[\"entity\"] = \"Knowledge\"\n",
    "                    del result[\"entity_group\"]\n",
    "                    entities.append(result)\n",
    "        \n",
    "        return {\"title\": email['subject'], \"text\": email['text_body'], \"entities\": entities}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c982bb06c5c6eb44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T08:28:12.004655500Z",
     "start_time": "2024-03-04T08:28:12.004655500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email =  [{\n",
    "    \"item_id\": 0,\n",
    "    \"sender\": \"a1d400258b5c6e3d97307b2c949ffe01fe0aa27ab02ef1c351a7bfa6e0f300a3\",\n",
    "    \"sender_email\": \"6fedc8e86e6e05504fefcdce51f8f73b69f5fd104c23dc5e9dba6c64e5536ffd\",\n",
    "    \"datetime_received\": 1707207587000,\n",
    "    \"sensitivity\": \"Normal\",\n",
    "    \"subject\": \"Datawarehousing Specialist (4. Expert (10+)) SWI000876 - For Swift\",\n",
    "    \"text_body\": \"ENKEL RECHTSTREEKS, GEEN TUSSENPARTIJEN AUB\\r\\n\\r\\nVOORRANG VASTE MEDEWERKERS\\r\\n\\r\\n\\r\\n\\r\\nHallo collega\\u2019s,\\r\\n\\r\\n\\r\\n\\r\\nVoor Swift zoeken we een Datawarehousing Specialist (4. Expert (10+)) SWI000876 die voldoet aan volgende beschrijving:\\r\\n\\r\\n\\r\\n\\r\\nUiterste reactiedatum: 16/02/2024\\r\\n\\r\\nGewenste startdatum: 01/03/2024\\r\\n\\r\\nEinddatum: 31/08/2024\\r\\n\\r\\n\\r\\n\\r\\nReferentie: SWI000876\\r\\n\\r\\nTitel: Datawarehousing Specialist (4. Expert (10+)) SWI000876\\r\\n\\r\\nLocatie: THE NETHERLANDS - ZOETERWOUDE (ENERGIEWEG 33, 2382 NC ZOETERWOUDE, NEDERLAND)\\r\\n\\r\\nStatus: Gepubliceerd\\r\\n\\r\\nType contract: Time & material\\r\\n\\r\\nCategorie: Niet van toepassing\\r\\n\\r\\nAantal personen: 1\\r\\n\\r\\nAfdeling: Human Resource (HR)\\r\\n\\r\\n\\r\\n\\r\\nOmschrijving\\r\\n\\r\\n\\r\\n\\r\\nThe project for which the candidate will be assigned is called Digital Dashboards, having the goal of building executive dashboarding for Swift. The ideal candidate will have an extensive background and expertise in MS Power BI, with both the ability to design the data model, as well as the reports and dashboards.\\r\\n\\r\\nThe candidate will join the project team and will have a key role to play, not just delivering on the scope of the project, but also training the team on that technology since it is still quite new at Swift.\\r\\n\\r\\n\\r\\nOpdracht informatie\\r\\n\\r\\n\\r\\n\\r\\nProjectnaam: Digital Dashboards\\r\\n\\r\\nWerkregime: Voltijds\\r\\n\\r\\n\\r\\n\\r\\nVaardigheden\\r\\n\\r\\n\\r\\n\\r\\nSPECIFIEKE VAARDIGHEDEN\\r\\n\\r\\n\\r\\n\\r\\nData Modeling: Expert (10+)\\r\\n\\r\\nETL Development: Expert (10+)\\r\\n\\r\\nMS Power Bi: Expert (10+)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nIndien jullie geschikte kandidaten hebben ontvang ik graag hun beschikbaarheid, CV en kostprijs.\\r\\n\\r\\n\\r\\n\\r\\nAlvast hartelijk bedankt.\\r\\n\\r\\n\\r\\n[signature_1929168496]\\r\\n\\r\\nChannice \\r\\n\\r\\nExecutive Assistant - Business and sales support\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nDe Cronos Groep nv\\r\\n\\r\\nVeldkant 33A, 2550 Kontich\\r\\n\\r\\n\\r\\n\",\n",
    "    \"label\": \"BI_ENGINEER\",\n",
    "    \"keywords\": [\n",
    "      \"Datawarehousing Specialist\",\n",
    "      \"MS Power BI\",\n",
    "      \"Data Modeling\",\n",
    "      \"ETL Development\"\n",
    "    ]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c25a41f523be4d3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T08:28:22.746045900Z",
     "start_time": "2024-03-04T08:28:12.650760Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "\n",
    "# loads in 6s and 660ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7faebae92cfc541c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T08:28:23.906996800Z",
     "start_time": "2024-03-04T08:28:22.746045900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'entity': 'B', 'score': 0.9989403, 'index': 2, 'word': 'researching', 'start': 6, 'end': 17}, {'entity': 'I', 'score': 0.99871325, 'index': 3, 'word': 'potential', 'start': 18, 'end': 27}, {'entity': 'I', 'score': 0.9916442, 'index': 4, 'word': 'solutions', 'start': 28, 'end': 37}, {'entity': 'B', 'score': 0.9979285, 'index': 35, 'word': 'developing', 'start': 194, 'end': 204}, {'entity': 'I', 'score': 0.9775203, 'index': 36, 'word': 'a', 'start': 205, 'end': 206}, {'entity': 'I', 'score': 0.9771836, 'index': 37, 'word': 'Python', 'start': 207, 'end': 213}, {'entity': 'I', 'score': 0.9831904, 'index': 38, 'word': 'program', 'start': 214, 'end': 221}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'entity': 'B', 'score': 0.999605, 'index': 13, 'word': 'collect', 'start': 52, 'end': 59}, {'entity': 'I', 'score': 0.99986744, 'index': 14, 'word': 'and', 'start': 60, 'end': 63}, {'entity': 'I', 'score': 0.99993, 'index': 15, 'word': 'display', 'start': 64, 'end': 71}, {'entity': 'I', 'score': 0.9999733, 'index': 16, 'word': 'real', 'start': 72, 'end': 76}, {'entity': 'I', 'score': 0.9999813, 'index': 17, 'word': '-', 'start': 76, 'end': 77}, {'entity': 'I', 'score': 0.99994504, 'index': 18, 'word': 'time', 'start': 77, 'end': 81}, {'entity': 'I', 'score': 0.9998348, 'index': 19, 'word': 'data', 'start': 82, 'end': 86}]\n",
      "[]\n",
      "[{'entity': 'B', 'score': 0.99997234, 'index': 8, 'word': 'I', 'start': 26, 'end': 27}, {'entity': 'B', 'score': 0.9999751, 'index': 9, 'word': '##mple', 'start': 27, 'end': 31}, {'entity': 'B', 'score': 0.9999819, 'index': 10, 'word': '##ment', 'start': 31, 'end': 35}, {'entity': 'I', 'score': 0.9999864, 'index': 11, 'word': 'a', 'start': 36, 'end': 37}, {'entity': 'I', 'score': 0.99998665, 'index': 12, 'word': 'system', 'start': 38, 'end': 44}, {'entity': 'B', 'score': 0.9986155, 'index': 14, 'word': 'send', 'start': 48, 'end': 52}, {'entity': 'I', 'score': 0.9999862, 'index': 15, 'word': 'alert', 'start': 53, 'end': 58}, {'entity': 'I', 'score': 0.9999865, 'index': 16, 'word': '##s', 'start': 58, 'end': 59}, {'entity': 'I', 'score': 0.9999856, 'index': 17, 'word': 'or', 'start': 60, 'end': 62}, {'entity': 'I', 'score': 0.9999865, 'index': 18, 'word': 'not', 'start': 63, 'end': 66}, {'entity': 'I', 'score': 0.9999784, 'index': 19, 'word': '##ifications', 'start': 66, 'end': 76}, {'entity': 'B', 'score': 0.99989986, 'index': 29, 'word': 'ensuring', 'start': 133, 'end': 141}, {'entity': 'I', 'score': 0.9999771, 'index': 30, 'word': 'pro', 'start': 142, 'end': 145}, {'entity': 'I', 'score': 0.9999789, 'index': 31, 'word': '##mpt', 'start': 145, 'end': 148}, {'entity': 'I', 'score': 0.99997485, 'index': 32, 'word': 'action', 'start': 149, 'end': 155}, {'entity': 'I', 'score': 0.99995387, 'index': 33, 'word': 'can', 'start': 156, 'end': 159}, {'entity': 'I', 'score': 0.9999759, 'index': 34, 'word': 'be', 'start': 160, 'end': 162}, {'entity': 'I', 'score': 0.99998033, 'index': 35, 'word': 'taken', 'start': 163, 'end': 168}]\n",
      "[]\n",
      "[{'entity': 'B', 'score': 0.99998605, 'index': 5, 'word': 'C', 'start': 14, 'end': 15}, {'entity': 'B', 'score': 0.9999875, 'index': 6, 'word': '##reate', 'start': 15, 'end': 20}, {'entity': 'I', 'score': 0.99997926, 'index': 7, 'word': 'a', 'start': 21, 'end': 22}, {'entity': 'I', 'score': 0.9999831, 'index': 8, 'word': 'database', 'start': 23, 'end': 31}, {'entity': 'I', 'score': 0.9999869, 'index': 9, 'word': 'or', 'start': 32, 'end': 34}, {'entity': 'I', 'score': 0.99998486, 'index': 10, 'word': 'log', 'start': 35, 'end': 38}, {'entity': 'I', 'score': 0.99998784, 'index': 11, 'word': 'file', 'start': 39, 'end': 43}, {'entity': 'B', 'score': 0.9997912, 'index': 13, 'word': 'store', 'start': 47, 'end': 52}, {'entity': 'I', 'score': 0.9999821, 'index': 14, 'word': 'historical', 'start': 53, 'end': 63}, {'entity': 'I', 'score': 0.9999833, 'index': 15, 'word': 'data', 'start': 64, 'end': 68}]\n",
      "[]\n",
      "[{'entity': 'B', 'score': 0.9999856, 'index': 8, 'word': 'Dev', 'start': 25, 'end': 28}, {'entity': 'B', 'score': 0.9999863, 'index': 9, 'word': '##elo', 'start': 28, 'end': 31}, {'entity': 'B', 'score': 0.9999888, 'index': 10, 'word': '##p', 'start': 31, 'end': 32}, {'entity': 'I', 'score': 0.9999881, 'index': 11, 'word': 'a', 'start': 33, 'end': 34}, {'entity': 'I', 'score': 0.99999034, 'index': 12, 'word': 'user', 'start': 35, 'end': 39}, {'entity': 'I', 'score': 0.99999166, 'index': 13, 'word': '-', 'start': 39, 'end': 40}, {'entity': 'I', 'score': 0.99999046, 'index': 14, 'word': 'friendly', 'start': 40, 'end': 48}, {'entity': 'I', 'score': 0.99999046, 'index': 15, 'word': 'interface', 'start': 49, 'end': 58}, {'entity': 'B', 'score': 0.7974014, 'index': 17, 'word': 'allows', 'start': 64, 'end': 70}, {'entity': 'I', 'score': 0.8715544, 'index': 18, 'word': 'easy', 'start': 71, 'end': 75}, {'entity': 'I', 'score': 0.9259511, 'index': 19, 'word': 'navigation', 'start': 76, 'end': 86}, {'entity': 'I', 'score': 0.7845666, 'index': 20, 'word': 'and', 'start': 87, 'end': 90}, {'entity': 'I', 'score': 0.81455433, 'index': 21, 'word': 'interaction', 'start': 91, 'end': 102}, {'entity': 'I', 'score': 0.9107097, 'index': 22, 'word': 'with', 'start': 103, 'end': 107}, {'entity': 'I', 'score': 0.9411245, 'index': 23, 'word': 'the', 'start': 108, 'end': 111}, {'entity': 'I', 'score': 0.925353, 'index': 24, 'word': 'program', 'start': 112, 'end': 119}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mail_1': {'title': 'Datawarehousing Specialist (4. Expert (10+)) SWI000876 - For Swift',\n",
       "  'keywords': []}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.preprocess_parallel(email)\n",
    "\n",
    "# classification takes 1s and 180 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71b07b974bff8850",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grimm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Grimm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time elapsed for parallel processing: 5.04219913482666s\n",
      "\n",
      "{'mail_1': {'title': 'Initial Inquiry - Pool Project', 'keywords': [{'word': 'researching potential solution', 'score': 0.9999105}, {'word': 'developing python program', 'score': 0.99930406}, {'word': 'collect display real time data', 'score': 0.99955183}, {'word': 'implement system', 'score': 0.9999465}, {'word': 'send alert notification', 'score': 0.9980915}, {'word': 'ensuring prompt action taken', 'score': 0.98590475}, {'word': 'logging', 'score': 0.98005944}, {'word': 'create database log file', 'score': 0.99996805}, {'word': 'store historical data', 'score': 0.9902846}, {'word': 'analysis', 'score': 0.8603323}, {'word': 'develop user friendly interface', 'score': 0.99995506}, {'word': 'python', 'score': 0.8881644}, {'word': 'python', 'score': 0.96697456}]}, 'mail_2': {'title': 'Follow Up - Pool Project Details', 'keywords': [{'word': 'availability', 'score': 0.88017213}]}, 'mail_3': {'title': 'Project Clarification - Detailed Specifications Attached', 'keywords': [{'word': 'prompt response', 'score': 0.9999083}, {'word': 'provide', 'score': 0.9110647}, {'word': 'python program development', 'score': 0.9992993}, {'word': 'python', 'score': 0.99785125}, {'word': 'data input source', 'score': 0.9995091}, {'word': 'expected output format', 'score': 0.97389156}, {'word': 'alert', 'score': 0.99932325}, {'word': 'data logging', 'score': 0.9998783}, {'word': 'preferred development framework', 'score': 0.98855966}]}, 'mail_4': {'title': 'Reintroducing Myself - New Opportunity', 'keywords': [{'word': 'researching potential solution', 'score': 0.999961}, {'word': 'developing python program', 'score': 0.99723035}, {'word': 'collect display real time data', 'score': 0.99957305}, {'word': 'implement system send alert notification', 'score': 0.9999552}, {'word': 'ensuring prompt action taken', 'score': 0.9981199}, {'word': 'logging', 'score': 0.85506827}, {'word': 'create database log file', 'score': 0.9999732}, {'word': 'store historical data', 'score': 0.9487961}, {'word': 'develop user friendly interface', 'score': 0.999908}, {'word': 'provide context', 'score': 0.9317529}, {'word': 'answer question', 'score': 0.8593467}, {'word': 'python', 'score': 0.9865176}, {'word': 'python', 'score': 0.954538}]}, 'mail_5': {'title': 'Follow Up - Additional Project Insights', 'keywords': [{'word': 'discus development python program', 'score': 0.9749459}, {'word': 'data logging', 'score': 0.9989525}]}, 'mail_6': {'title': 'Providing Complete Project Overview - Ready For Collaboration', 'keywords': [{'word': 'prompt response', 'score': 0.9999199}, {'word': 'python program development', 'score': 0.99947697}, {'word': 'python', 'score': 0.9972024}, {'word': 'data input source', 'score': 0.99937516}, {'word': 'alert', 'score': 0.9991122}, {'word': 'data logging', 'score': 0.9998944}, {'word': 'project specification', 'score': 0.9911964}]}}\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, model_dir):\n",
    "        self.model_dir = model_dir\n",
    "        self.token_skill_classifier = None\n",
    "        self.token_knowledge_classifier = None\n",
    "        self.load_models()\n",
    "\n",
    "    def load_models(self):\n",
    "        skill_model_path = os.path.join(self.model_dir, \"skill_model.joblib\")\n",
    "        knowledge_model_path = os.path.join(self.model_dir, \"knowledge_model.joblib\")\n",
    "\n",
    "        if os.path.exists(skill_model_path):\n",
    "            self.token_skill_classifier = load(skill_model_path)\n",
    "\n",
    "        if os.path.exists(knowledge_model_path):\n",
    "            self.token_knowledge_classifier = load(knowledge_model_path)\n",
    "\n",
    "        if self.token_skill_classifier is None:\n",
    "            self.token_skill_classifier = pipeline(model=\"jjzha/jobbert_skill_extraction\", aggregation_strategy=\"first\")\n",
    "            dump(self.token_skill_classifier, skill_model_path)\n",
    "\n",
    "        if self.token_knowledge_classifier is None:\n",
    "            self.token_knowledge_classifier = pipeline(model=\"jjzha/jobbert_knowledge_extraction\", aggregation_strategy=\"first\")\n",
    "            dump(self.token_knowledge_classifier, knowledge_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
